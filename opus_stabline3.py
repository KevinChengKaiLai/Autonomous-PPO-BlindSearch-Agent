import numpy as np
import matplotlib.pyplot as plt
import gymnasium as gym
from gymnasium import spaces
from stable_baselines3 import PPO, DQN, SAC
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.callbacks import BaseCallback, EvalCallback
from stable_baselines3.common.monitor import Monitor
from typing import Dict, List, Tuple, Optional
import torch
from collections import deque
import matplotlib
matplotlib.use('Agg')  # éäº’å‹•å¼å¾Œç«¯ï¼Œé©åˆColab

# ===== è‡ªå®šç¾© Gym ç’°å¢ƒ =====

class BlindSearchEnv(gym.Env):
    """ç›²ç›®æœç´¢ç’°å¢ƒ - ç¬¦åˆ Gymnasium æ¥å£"""
    
    def __init__(self, grid_size: int = 50, sigma: float = 0.05, max_steps: int = 5000):
        super().__init__()
        self.grid_size = grid_size
        self.sigma = sigma
        self.max_steps = max_steps
        
        # å®šç¾©å‹•ä½œç©ºé–“ï¼š8å€‹é›¢æ•£æ–¹å‘
        self.action_space = spaces.Discrete(8)
        
        # å®šç¾©è§€å¯Ÿç©ºé–“
        # [agent_x, agent_y, visit_map(10x10), time_ratio, last_direction(8)]
        self.observation_space = spaces.Box(
            low=0, high=1, 
            shape=(2 + 100 + 1 + 8,), 
            dtype=np.float32
        )
        
        # æ–¹å‘å‘é‡
        self.directions = np.array([
            [1, 0], [1, 1], [0, 1], [-1, 1],
            [-1, 0], [-1, -1], [0, -1], [1, -1]
        ], dtype=np.float32) 

        
        norms = np.linalg.norm(self.directions, axis=1, keepdims=True)
        self.directions = self.directions / norms
        
        self.reset()
        
    def reset(self, seed=None, options=None):
        """reset environment"""
        super().reset(seed=seed)
        
        # Agent å¾å·¦ä¸‹è§’é–‹å§‹
        self.agent_pos = np.array([0.0, 0.0])
        
        # éš¨æ©Ÿç”Ÿæˆç›®æ¨™ä½ç½®
        margin = 0
        self.target_pos = np.array([
            self.np_random.uniform(margin, 1.0 - margin),
            self.np_random.uniform(margin, 1.0 - margin)
        ])
        
        # åˆå§‹åŒ–è¨ªå•åœ°åœ–å’Œè»Œè·¡
        self.visited = np.zeros((self.grid_size, self.grid_size))
        self.trajectory = [self.agent_pos.copy()]
        self.steps = 0
        self.last_direction = 0
        
        # è¨˜éŒ„åˆå§‹ä½ç½®ç‚ºå·²è¨ªå•
        self._mark_visited(self.agent_pos)
        
        return self._get_observation(), {}
    
    def step(self, action: int):
        """execute action"""
        # ç§»å‹•é€Ÿåº¦
        speed = 2.0 / self.grid_size
        
        # æ›´æ–°ä½ç½®
        new_pos = self.agent_pos + self.directions[action] * speed
        new_pos = np.clip(new_pos, 0, 1)
        
        # è¨ˆç®—ç§»å‹•å‰çš„è¨ªå•ç‹€æ…‹
        old_coverage = np.sum(self.visited > 0)
        
        self.agent_pos = new_pos
        self.trajectory.append(self.agent_pos.copy())
        self.steps += 1
        self.last_direction = action
        
        # æ›´æ–°è¨ªå•åœ°åœ–
        is_new_cell = self._mark_visited(self.agent_pos)
        new_coverage = np.sum(self.visited > 0)
        
        # æª¢æŸ¥æ˜¯å¦æ‰¾åˆ°ç›®æ¨™
        distance = np.linalg.norm(self.agent_pos - self.target_pos)
        found = distance < self.sigma
        
        # è¨ˆç®—çå‹µ
        reward = self._calculate_reward(
            found, distance, is_new_cell, 
            old_coverage, new_coverage
        )
        
        # çµ‚æ­¢æ¢ä»¶
        terminated = found
        truncated = self.steps >= self.max_steps
        
        info = {
            'distance': distance,
            'coverage': new_coverage / (self.grid_size ** 2),
            'success': found,
            'steps': self.steps
        }
        
        return self._get_observation(), reward, terminated, truncated, info
    
    def _calculate_reward(self, found: bool, distance: float, 
                         is_new_cell: bool, old_coverage: int, 
                         new_coverage: int) -> float:
        """calculate reward function"""
        if found:
            # æ‰¾åˆ°ç›®æ¨™çš„çå‹µï¼Œæ ¹æ“šæ­¥æ•¸çµ¦äºˆé¡å¤–çå‹µ
            base_reward = 1000
            efficiency_bonus = max(0, (self.max_steps - self.steps) / self.max_steps) * 500
            return base_reward + efficiency_bonus
        
        # åŸºç¤æ­¥æ•¸æ‡²ç½°
        reward = -1
        
        # æ¢ç´¢æ–°å€åŸŸçå‹µ
        if is_new_cell:
            reward += 5
        
        # è¦†è“‹ç‡å¢é•·çå‹µ
        coverage_increase = (new_coverage - old_coverage) / (self.grid_size ** 2)
        reward += coverage_increase * 100
        
        # è·é›¢ç›®æ¨™çš„éš±å¼çå‹µï¼ˆç”¨æ–¼å¼•å°ï¼Œä½†ä¸èƒ½ç›´æ¥æ„ŸçŸ¥ç›®æ¨™ï¼‰
        # é€™å€‹åœ¨å¯¦éš›ç›²ç›®æœç´¢ä¸­ä¸æ‡‰è©²æœ‰ï¼Œä½†æœ‰åŠ©æ–¼åŠ é€Ÿè¨“ç·´
        # reward -= distance * 0.1
        
        return reward
    
    def _mark_visited(self, pos: np.ndarray) -> bool:
        """mark visited position, return if it's a new cell"""
        grid_x = int(pos[0] * self.grid_size)
        grid_y = int(pos[1] * self.grid_size)
        grid_x = min(grid_x, self.grid_size - 1)
        grid_y = min(grid_y, self.grid_size - 1)
        
        is_new = self.visited[grid_y, grid_x] == 0
        self.visited[grid_y, grid_x] += 1
        
        return is_new
    
    def _get_observation(self) -> np.ndarray:
        """get observation"""
        obs = []
        
        # Agent ä½ç½®
        obs.extend(self.agent_pos.tolist())
        
        # è¨ªå•åœ°åœ–ï¼ˆé™æ¡æ¨£åˆ° 10x10ï¼‰        
        # ğŸ”§ æ”¹é€²ï¼šè¨ªå•åœ°åœ–æ­¸ä¸€åŒ–ï¼Œä¿ç•™é »ç‡ä¿¡æ¯
        visit_map = self.visited.reshape(10, 5, 10, 5).mean(axis=(1, 3))
        max_visits = np.max(visit_map) if np.max(visit_map) > 0 else 1
        visit_map = np.clip(visit_map / max_visits, 0, 1)

        obs.extend(visit_map.flatten().tolist())
        # æ™‚é–“æ¯”ä¾‹
        obs.append(self.steps / self.max_steps)
        
        # ä¸Šä¸€å€‹æ–¹å‘ï¼ˆone-hotï¼‰
        direction_onehot = [0] * 8
        direction_onehot[self.last_direction] = 1
        obs.extend(direction_onehot)
        
        return np.array(obs, dtype=np.float32)
    
    def render(self, mode='human'):
        """render environment"""
        if not hasattr(self, 'fig'):
            self.fig, self.ax = plt.subplots(figsize=(8, 8))
            # ç§»é™¤ plt.ion() - åœ¨Colabä¸­ä¸éœ€è¦
        
        self.ax.clear()
        
        # ç¹ªè£½è¨ªå•ç†±åŠ›åœ–
        self.ax.imshow(self.visited.T, cmap='Blues', alpha=0.5, 
                      origin='lower', extent=[0, 1, 0, 1])
        
        # ç¹ªè£½è»Œè·¡
        if len(self.trajectory) > 1:
            traj = np.array(self.trajectory)
            self.ax.plot(traj[:, 0], traj[:, 1], 'b-', alpha=0.7, linewidth=2)
        
        # ç¹ªè£½ Agent
        self.ax.plot(self.agent_pos[0], self.agent_pos[1], 'ro', markersize=10)
        
        # ç¹ªè£½ç›®æ¨™
        circle = plt.Circle(self.target_pos, self.sigma, color='green', 
                          fill=False, linewidth=2)
        self.ax.add_patch(circle)
        self.ax.plot(self.target_pos[0], self.target_pos[1], 'g*', markersize=15)
        
        self.ax.set_xlim(0, 1)
        self.ax.set_ylim(0, 1)
        self.ax.set_aspect('equal')
        self.ax.set_title(f'Steps: {self.steps} | Coverage: {np.sum(self.visited > 0) / (self.grid_size ** 2):.1%}')
        self.ax.grid(True, alpha=0.3)
        
        # ç§»é™¤ plt.pause() - åœ¨Colabä¸­æœƒå¡ä½
        plt.draw()
        plt.show()  # æ”¹ç‚ºç›´æ¥é¡¯ç¤º
        
    def close(self):
        """close environment"""
        if hasattr(self, 'fig'):
            plt.close(self.fig)

# ===== è‡ªå®šç¾©å›èª¿å‡½æ•¸ =====

class TrajectoryCollectorCallback(BaseCallback):
    """collect successful trajectories callback"""
    
    def __init__(self, verbose=0):
        super().__init__(verbose)
        self.successful_trajectories = []
        self.episode_count = 0
        self.success_count = 0
        
    def _on_step(self) -> bool:
        # æª¢æŸ¥æ˜¯å¦æœ‰ç’°å¢ƒå®Œæˆ
        if self.locals.get('dones')[0]:
            info = self.locals.get('infos')[0]
            self.episode_count += 1
            
            # å¦‚æœæˆåŠŸæ‰¾åˆ°ç›®æ¨™ï¼Œä¿å­˜è»Œè·¡
            if info.get('success', False):
                self.success_count += 1
                # ç²å–åº•å±¤ç’°å¢ƒï¼ˆç¹éMonitoråŒ…è£ï¼‰
                env = self.training_env.envs[0]
                if hasattr(env, 'env'):  # å¦‚æœæ˜¯MonitoråŒ…è£çš„ç’°å¢ƒ
                    env = env.env
                self.successful_trajectories.append({
                    'trajectory': env.trajectory.copy(),
                    'steps': info['steps'],
                    'target': env.target_pos.copy()
                })
                
                if self.verbose > 0 and self.success_count % 10 == 0:
                    print(f"Collected {self.success_count} successful trajectories")
        
        return True

class ProgressCallback(BaseCallback):
    """training progress callback"""
    
    def __init__(self, check_freq=1000, verbose=1):
        super().__init__(verbose)
        self.check_freq = check_freq
        self.episode_rewards = []
        self.episode_lengths = []
        
    def _on_step(self) -> bool:
        if self.n_calls % self.check_freq == 0:
            # ç²å–è¨“ç·´çµ±è¨ˆ
            if len(self.model.ep_info_buffer) > 0:
                mean_reward = np.mean([ep_info['r'] for ep_info in self.model.ep_info_buffer])
                mean_length = np.mean([ep_info['l'] for ep_info in self.model.ep_info_buffer])
                
                self.episode_rewards.append(mean_reward)
                self.episode_lengths.append(mean_length)
                
                if self.verbose > 0:
                    print(f"Steps: {self.n_calls} | Mean Reward: {mean_reward:.2f} | Mean Length: {mean_length:.2f}")
        
        return True

# ===== è¨“ç·´å‡½æ•¸ =====

import os
import pickle

def train_blind_search_agent(total_timesteps: int = 500000, 
                           algorithm: str = 'PPO',
                           render_freq: int = 0):
    """train blind search agent"""
    
    print(f"=== use Stable-Baselines3 {algorithm} to train blind search agent ===\n")
    
    if torch.cuda.is_available():
        device = 'cuda'
        print(f"âœ… Using GPU: {torch.cuda.get_device_name()}")
    else:
        device = 'cpu'
        print("âš ï¸  GPU not available, using CPU")
    
    # å‰µå»ºç’°å¢ƒ - ä½¿ç”¨å›ºå®š seed åºåˆ—
    env = BlindSearchEnv(grid_size=50, sigma=0.05, max_steps=5000)
    env = Monitor(env)
    env = DummyVecEnv([lambda: env])
    
    # å‰µå»ºè©•ä¼°ç’°å¢ƒ - ä½¿ç”¨ç›¸åŒçš„å›ºå®š seed åºåˆ—
    eval_env = BlindSearchEnv(grid_size=50, sigma=0.05, max_steps=5000)
    eval_env = Monitor(eval_env)
    eval_env = DummyVecEnv([lambda: eval_env])
    
    # é¸æ“‡ç®—æ³•
    if algorithm == 'PPO':
        model = PPO(
            'MlpPolicy',
            env,
            device=device,
            learning_rate=3e-4,
            n_steps=4096,
            batch_size=128,
            n_epochs=10,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            ent_coef=0.01,
            vf_coef=0.5,
            max_grad_norm=0.5,
            use_sde=False,
            policy_kwargs=dict(
                net_arch=[256, 256, 256],
                activation_fn=torch.nn.ReLU
            ),
            verbose=1,
            tensorboard_log="./blind_search_tensorboard/")
    elif algorithm == 'DQN':
        model = DQN(
            'MlpPolicy',
            env,
            learning_rate=1e-3,
            buffer_size=100000,
            learning_starts=1000,
            batch_size=32,
            tau=1.0,
            gamma=0.99,
            train_freq=4,
            target_update_interval=1000,
            verbose=1,
            tensorboard_log="./blind_search_tensorboard/"
        )
    elif algorithm == 'SAC':
        model = SAC(
            'MlpPolicy',
            env,
            learning_rate=3e-4,
            buffer_size=100000,
            learning_starts=1000,
            batch_size=256,
            tau=0.005,
            gamma=0.99,
            verbose=1,
            tensorboard_log="./blind_search_tensorboard/"
        )
    else:
        raise ValueError(f"Unknown algorithm: {algorithm}")
    
    # å‰µå»ºå›èª¿å‡½æ•¸
    trajectory_callback = TrajectoryCollectorCallback(verbose=1)
    progress_callback = ProgressCallback(check_freq=5000)
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path='./blind_search_best_model/',
        log_path='./blind_search_logs/',
        eval_freq=10000,
        deterministic=True,
        render=False
    )
    
    callbacks = [trajectory_callback, progress_callback, eval_callback]
    
    # è¨“ç·´æ¨¡å‹
    print("start training...")
    model.learn(
        total_timesteps=total_timesteps,
        callback=callbacks,
        progress_bar=True
    )
    
    print(f"\ntraining done! collected {len(trajectory_callback.successful_trajectories)} successful trajectories")
    
    # ä¿å­˜æ¨¡å‹åˆ°æœ¬åœ°
    model_path = f"blind_search_{algorithm.lower()}_final"
    model.save(model_path)
    print(f"æ¨¡å‹å·²ä¿å­˜åˆ°æœ¬åœ°: {model_path}")
    
    return model, trajectory_callback.successful_trajectories, progress_callback, model_path

# ===== è©•ä¼°å‡½æ•¸ =====

def evaluate_agent(model, n_eval_episodes: int = 100, render: bool = False):
    """evaluate trained agent"""
    
    print("\n=== evaluate agent performance ===")
    
    # ä½¿ç”¨å¤šå€‹ä¸åŒçš„ seed ç¯„åœé€²è¡Œè©•ä¼°
    seed_ranges = [
        (0, 99),      # è¨“ç·´æ™‚çœ‹åˆ°çš„ seed ç¯„åœ
        (100, 199),   # æ–°çš„ seed ç¯„åœ
        (200, 299),   # å¦ä¸€å€‹æ–°çš„ seed ç¯„åœ
    ]
    
    all_results = []
    
    for start_seed, end_seed in seed_ranges:
        env = BlindSearchEnv(grid_size=50, sigma=0.05, max_steps=5000)
        
        successes = 0
        steps_list = []
        
        for episode in range(n_eval_episodes):
            seed = start_seed + episode
            obs, _ = env.reset(seed=seed)
            done = False
            
            while not done:
                action, _ = model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = env.step(action)
                done = terminated or truncated
                
                if render and episode < 5:
                    env.render()
            
            if info['success']:
                successes += 1
                steps_list.append(info['steps'])
        
        results = {
            'seed_range': (start_seed, end_seed),
            'success_rate': successes / n_eval_episodes,
            'avg_steps': np.mean(steps_list) if steps_list else float('inf')
        }
        all_results.append(results)
        env.close()
    
    # è¨ˆç®—å¹³å‡çµæœ
    avg_success_rate = np.mean([r['success_rate'] for r in all_results])
    print(f"å¤šç¯„åœè©•ä¼°å¹³å‡æˆåŠŸç‡: {avg_success_rate:.2%}")
    
    return all_results

# ===== è»Œè·¡åˆ†æå‡½æ•¸ =====

def analyze_trajectories(trajectories: List[Dict], model=None):
    """åˆ†æå­¸ç¿’åˆ°çš„è»Œè·¡"""
    
    print(f"\n=== analyze {len(trajectories)} successful trajectories ===")
    
    if len(trajectories) == 0:
        print("no successful trajectories to analyze")
        return
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    # 1. è»Œè·¡ç–ŠåŠ åœ–
    ax = axes[0, 0]
    for i, traj_data in enumerate(trajectories[:30]):  # é¡¯ç¤ºå‰30æ¢
        traj = np.array(traj_data['trajectory'])
        ax.plot(traj[:, 0], traj[:, 1], alpha=0.3, linewidth=1)
    ax.set_title('overlay trajectories (top 30)')
    ax.set_aspect('equal')
    ax.grid(True)
    ax.set_xlim(0, 1)
    ax.set_ylim(0, 1)
    
    # 2. å¹³å‡è¨ªå•ç†±åŠ›åœ–
    ax = axes[0, 1]
    grid_size = 50
    avg_visited = np.zeros((grid_size, grid_size))
    
    for traj_data in trajectories:
        visited = np.zeros((grid_size, grid_size))
        traj = np.array(traj_data['trajectory'])
        for pos in traj:
            x = int(pos[0] * grid_size)
            y = int(pos[1] * grid_size)
            x = min(x, grid_size - 1)
            y = min(y, grid_size - 1)
            visited[y, x] = 1
        avg_visited += visited
    
    avg_visited /= len(trajectories)
    im = ax.imshow(avg_visited.T, cmap='hot', origin='lower', extent=[0, 1, 0, 1])
    ax.set_title('mean visited heatmap')
    ax.set_aspect('equal')
    plt.colorbar(im, ax=ax)
    
    # 3. æ­¥æ•¸åˆ†å¸ƒ
    ax = axes[1, 0]
    steps = [t['steps'] for t in trajectories]
    ax.hist(steps, bins=30, alpha=0.7, edgecolor='black')
    ax.set_xlabel('steps')
    ax.set_ylabel('frequency')
    ax.set_title(f'steps distribution (mean: {np.mean(steps):.1f})')
    ax.grid(True, alpha=0.3)
    
    # 4. ç›®æ¨™ä½ç½®åˆ†å¸ƒ
    ax = axes[1, 1]
    targets = np.array([t['target'] for t in trajectories])
    scatter = ax.scatter(targets[:, 0], targets[:, 1], 
                        c=[t['steps'] for t in trajectories],
                        cmap='viridis', alpha=0.6)
    ax.set_xlabel('target X')
    ax.set_ylabel('target Y')
    ax.set_title('target position vs steps')
    ax.set_aspect('equal')
    ax.set_xlim(0, 1)
    ax.set_ylim(0, 1)
    plt.colorbar(scatter, ax=ax, label='steps')
    
    plt.tight_layout()
    plt.show()
    
    # å¦‚æœæœ‰æ¨¡å‹ï¼Œå±•ç¤ºä¸€å€‹ç¤ºä¾‹é‹è¡Œ
    if model is not None:
        print("\nshow a test run...")
        env = BlindSearchEnv(grid_size=50, sigma=0.05, max_steps=5000)
        obs, _ = env.reset()  # æ¯å€‹ episode ä½¿ç”¨ä¸åŒçš„å›ºå®š seed
        
        for step in range(5000):
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            
            if step % 100 == 0:
                env.render()
            
            if terminated or truncated:
                print(f"done! steps: {info['steps']}, success: {info['success']}")
                break
        
        env.render()
        # ç§»é™¤ input() - åœ¨Colabä¸­ç„¡æ³•ä½¿ç”¨
        print("Test run completed!")
        env.close()

# ===== ä¸»ç¨‹åº =====

def main():
    """ä¸»ç¨‹åºï¼šè¨“ç·´å’Œè©•ä¼°ç›²ç›®æœç´¢æ™ºèƒ½é«”"""
    
    # è¨“ç·´åƒæ•¸
    TOTAL_TIMESTEPS = 1e7
    ALGORITHM = 'PPO'
    
    # è¨“ç·´æ¨¡å‹
    model, trajectories, progress_callback, model_path = train_blind_search_agent(
        total_timesteps=TOTAL_TIMESTEPS,
        algorithm=ALGORITHM
    )
    
    # è©•ä¼°æ¨¡å‹
    results = evaluate_agent(model, n_eval_episodes=100, render=False)
    
    # ä¿å­˜çµæœæ•¸æ“šåˆ°æœ¬åœ°
    results_data = {
        'results': results,
        'trajectories_count': len(trajectories),
        'model_path': model_path,
        'algorithm': ALGORITHM,
        'timesteps': TOTAL_TIMESTEPS
    }
    
    results_path = 'training_results.pkl'
    with open(results_path, 'wb') as f:
        pickle.dump(results_data, f)
    
    print(f"è¨“ç·´çµæœå·²ä¿å­˜åˆ°æœ¬åœ°: {results_path}")
    print(f"æ¨¡å‹è·¯å¾‘: {model_path}")
    
    return model, trajectories, results, model_path

# ===== æ¯”è¼ƒä¸åŒç®—æ³• =====

def compare_algorithms():
    """æ¯”è¼ƒä¸åŒRLç®—æ³•çš„æ€§èƒ½"""
    
    algorithms = ['PPO', 'DQN', 'SAC']
    results = {}
    
    for algo in algorithms:
        print(f"\n{'='*50}")
        print(f"è¨“ç·´ {algo}")
        print(f"{'='*50}")
        
        model, trajectories, _ = train_blind_search_agent(
            total_timesteps=200000,  # è¼ƒå°‘çš„æ­¥æ•¸ç”¨æ–¼å¿«é€Ÿæ¯”è¼ƒ
            algorithm=algo
        )
        
        results[algo] = evaluate_agent(model, n_eval_episodes=50)
        results[algo]['trajectories'] = len(trajectories)
    
    # å¯è¦–åŒ–æ¯”è¼ƒçµæœ
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
    algos = list(results.keys())
    success_rates = [results[a]['success_rate'] for a in algos]
    avg_steps = [results[a]['avg_steps'] for a in algos]
    
    ax1.bar(algos, success_rates)
    ax1.set_ylabel('success rate')
    ax1.set_title('algorithm success rate comparison')
    ax1.set_ylim(0, 1)
    
    ax2.bar(algos, avg_steps)
    ax2.set_ylabel('mean steps')
    ax2.set_title('mean steps comparison')
    
    plt.tight_layout()
    plt.show()
    
    return results

if __name__ == "__main__":
    # é‹è¡Œä¸»ç¨‹åº
    model, trajectories, results, model_path = main()
    
    # å¦‚æœæƒ³æ¯”è¼ƒä¸åŒç®—æ³•ï¼Œå–æ¶ˆè¨»é‡‹ä¸‹ä¸€è¡Œ
    # comparison_results = compare_algorithms()